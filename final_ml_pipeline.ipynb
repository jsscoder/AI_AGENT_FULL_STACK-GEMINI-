{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsscoder/AI_AGENT_FULL_STACK-GEMINI-/blob/main/final_ml_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEvO9dYfgaX9"
      },
      "source": [
        "BISAG N GEOSPATIAL DATA ANALYSIS AND SEMANTIC SEGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCtasT-Cfj_p"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsalNEKsD1aQ"
      },
      "source": [
        "This full code does the following:\n",
        "\n",
        "Generates cluster masks automatically if they don’t exist.\n",
        "\n",
        "Loads the dataset and resizes images/masks.\n",
        "\n",
        "Defines a UNet model for multi-class segmentation.\n",
        "\n",
        "Trains the model with CrossEntropyLoss.\n",
        "\n",
        "Evaluates using classification report and confusion matrix.\n",
        "\n",
        "Visualizes predictions with color-coded masks and text labels."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEPENDENCY ELICITATION PART 1"
      ],
      "metadata": {
        "id": "2wK08BZ-XDi5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doUD0GiGRw0J",
        "outputId": "f2ff9e64-db41-467c-affb-eb9f5d353bc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch torchvision torchaudio numpy opencv-python pillow scikit-learn tqdm matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEPENDENCY ELICITATION PART 2"
      ],
      "metadata": {
        "id": "HXCw62X6WbrB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Er8oh54gUg",
        "outputId": "0fc67cf5-b73d-4df1-e264-55ef51cb4fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m955.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imagecodecs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOwyX28tSoX8",
        "outputId": "7f975892-f34c-4d51-f5a7-f38818b9ba9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imagecodecs in /usr/local/lib/python3.12/dist-packages (2025.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from imagecodecs) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FORMULATING AND TRANING THE PIPELINE"
      ],
      "metadata": {
        "id": "rwMsXKHMWf6o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c--XYDWfD1Wx",
        "outputId": "5afa57b0-6157-426e-cefe-df75e65c47c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Building dataset...\n",
            "Found 1 usable images (images with masks).\n",
            "n_classes (detected): 5\n",
            "label values found: [0, 1, 2, 3, 4]\n",
            "label_to_index mapping: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
            "Using class names: ['background', 'water', 'land', 'vegetation', 'barren']\n",
            "Single-image dataset detected, skipping train/val split.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/30]: 100%|██████████| 1/1 [00:03<00:00,  3.20s/it, loss=1.8001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 — Train: 1.8001  Val: 1.6183\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/30]: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it, loss=1.3329]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 02 — Train: 1.3329  Val: 1.6066\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/30]: 100%|██████████| 1/1 [00:02<00:00,  2.34s/it, loss=1.0570]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 03 — Train: 1.0570  Val: 1.5906\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [4/30]: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it, loss=0.9201]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 04 — Train: 0.9201  Val: 1.5694\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [5/30]: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it, loss=0.8444]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 05 — Train: 0.8444  Val: 1.5380\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [6/30]: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it, loss=0.7918]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 06 — Train: 0.7918  Val: 1.4862\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [7/30]: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it, loss=0.7462]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 07 — Train: 0.7462  Val: 1.4187\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [8/30]: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it, loss=0.7077]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 08 — Train: 0.7077  Val: 1.3630\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [9/30]: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it, loss=0.6737]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 09 — Train: 0.6737  Val: 1.2575\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [10/30]: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it, loss=0.6396]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 — Train: 0.6396  Val: 1.1602\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [11/30]: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it, loss=0.6309]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 — Train: 0.6309  Val: 1.0181\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [12/30]: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it, loss=0.6056]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 — Train: 0.6056  Val: 0.9606\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [13/30]: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it, loss=0.5756]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 — Train: 0.5756  Val: 0.9294\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [14/30]: 100%|██████████| 1/1 [00:02<00:00,  2.62s/it, loss=0.5484]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 — Train: 0.5484  Val: 0.9225\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [15/30]: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it, loss=0.5290]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 — Train: 0.5290  Val: 0.9053\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [16/30]: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it, loss=0.5059]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 — Train: 0.5059  Val: 0.8276\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [17/30]: 100%|██████████| 1/1 [00:02<00:00,  2.27s/it, loss=0.4877]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 — Train: 0.4877  Val: 0.7705\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [18/30]: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it, loss=0.4700]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 — Train: 0.4700  Val: 0.7502\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [19/30]: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it, loss=0.4515]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 — Train: 0.4515  Val: 0.7194\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [20/30]: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it, loss=0.4357]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 — Train: 0.4357  Val: 0.6779\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [21/30]: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it, loss=0.4193]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 — Train: 0.4193  Val: 0.6472\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [22/30]: 100%|██████████| 1/1 [00:02<00:00,  2.30s/it, loss=0.4041]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 — Train: 0.4041  Val: 0.6242\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [23/30]: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it, loss=0.3909]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 — Train: 0.3909  Val: 0.5917\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [24/30]: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it, loss=0.3771]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 — Train: 0.3771  Val: 0.5655\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [25/30]: 100%|██████████| 1/1 [00:02<00:00,  2.66s/it, loss=0.3646]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 — Train: 0.3646  Val: 0.5546\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [26/30]: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it, loss=0.3526]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 — Train: 0.3526  Val: 0.5363\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [27/30]: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it, loss=0.3410]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 — Train: 0.3410  Val: 0.5193\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [28/30]: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it, loss=0.3301]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 — Train: 0.3301  Val: 0.4934\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [29/30]: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it, loss=0.3189]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 — Train: 0.3189  Val: 0.4687\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [30/30]: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it, loss=0.3091]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 — Train: 0.3091  Val: 0.4519\n",
            "✅ Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved per-class metrics to outputs/per_class_metrics.csv\n",
            "Saved per-image area (m²) to outputs/per_image_area_m2.csv\n",
            "Generating side-by-side overlays with GT, Predicted, and Overlay...\n",
            "✅ Saved example overlays to outputs\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Hardened segmentation training + evaluation pipeline\n",
        "Save as hardened_segmentation_pipeline_v2.py and run in your training environment.\n",
        "Key features:\n",
        " - Robust TIFF loading via tifffile (fallback to PIL/OpenCV)\n",
        " - Mask raw-value -> contiguous-index remapping\n",
        " - Filter images without matching masks at Dataset init\n",
        " - Safer DataLoader worker usage during debugging (NUM_WORKERS=0 default)\n",
        " - Conditional AMP scaler only when CUDA available\n",
        " - Auto-scale UNet base filters by available GPU memory\n",
        " - Checkpointing and resume support\n",
        " - Memory-safe confusion matrix (bincount)\n",
        " - Skip metric/visual generation when no predictions\n",
        " - CSV logging for train/val losses and per-class metrics\n",
        " - Stable plotting and deduped correlation heatmap\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# optionally import tifffile if available\n",
        "try:\n",
        "    import tifffile\n",
        "    _HAS_TIFFFILE = True\n",
        "except Exception:\n",
        "    tifffile = None\n",
        "    _HAS_TIFFFILE = False\n",
        "\n",
        "# -----------------------\n",
        "# Config (edit as needed)\n",
        "# -----------------------\n",
        "IMG_DIR = \"dataset/images\"   # adjust\n",
        "MASK_DIR = \"dataset/masks\"   # adjust\n",
        "OUT_DIR  = \"outputs\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "VAL_SPLIT = 0.2\n",
        "N_EPOCHS = 30\n",
        "ACCUM_STEPS = 4\n",
        "LR = 1e-3\n",
        "NUM_WORKERS = 8  # safer during debugging; set >0 when loader is stable\n",
        "\n",
        "# Optional: provide friendly class names in order of label values (0..N-1)\n",
        "CLASS_NAMES = [\"background\", \"water\", \"land\", \"vegetation\", \"barren\", \"built_up\"]\n",
        "\n",
        "# -----------------------\n",
        "# Device\n",
        "# -----------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# -----------------------\n",
        "# Utility: find mask filename for an image\n",
        "# -----------------------\n",
        "\n",
        "def find_mask_for_image(img_name, mask_dir):\n",
        "    cand = os.path.join(mask_dir, img_name)\n",
        "    if os.path.exists(cand):\n",
        "        return cand\n",
        "    base, _ = os.path.splitext(img_name)\n",
        "    for ext in [\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\", \".bmp\"]:\n",
        "        cand = os.path.join(mask_dir, base + ext)\n",
        "        if os.path.exists(cand):\n",
        "            return cand\n",
        "    raise FileNotFoundError(f\"Mask for image {img_name} not found in {mask_dir}\")\n",
        "\n",
        "# -----------------------\n",
        "# Helper: robust image loader (returns PIL.Image)\n",
        "# -----------------------\n",
        "\n",
        "def _load_image_with_tiff_support(path):\n",
        "    path = str(path)\n",
        "    # prefer tifffile for .tif / .tiff\n",
        "    ext = os.path.splitext(path)[1].lower()\n",
        "    if ext in ('.tif', '.tiff') and _HAS_TIFFFILE:\n",
        "        try:\n",
        "            arr = tifffile.imread(path)\n",
        "            # if shape is (C,H,W), transpose\n",
        "            if arr.ndim == 3 and arr.shape[0] <= 8 and arr.shape[0] != arr.shape[2]:\n",
        "                arr = np.transpose(arr, (1,2,0))\n",
        "            # single band -> make 3-channel\n",
        "            if arr.ndim == 2:\n",
        "                arr = np.stack([arr]*3, axis=-1)\n",
        "            # if more than 3 bands, take first 3\n",
        "            if arr.ndim == 3 and arr.shape[2] > 3:\n",
        "                arr = arr[..., :3]\n",
        "            # normalize/convert to uint8 if needed\n",
        "            if arr.dtype == np.float32 or arr.dtype == np.float64:\n",
        "                if arr.max() <= 1.0:\n",
        "                    arr = (arr * 255).astype(np.uint8)\n",
        "                else:\n",
        "                    arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
        "            elif not np.issubdtype(arr.dtype, np.uint8):\n",
        "                arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
        "            return Image.fromarray(arr)\n",
        "        except Exception:\n",
        "            # fallback to other loaders below\n",
        "            pass\n",
        "    # try PIL\n",
        "    try:\n",
        "        return Image.open(path)\n",
        "    except UnidentifiedImageError:\n",
        "        # try OpenCV as fallback\n",
        "        arr = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
        "        if arr is None:\n",
        "            raise\n",
        "        # OpenCV returns BGR\n",
        "        if arr.ndim == 3:\n",
        "            if arr.shape[2] > 3:\n",
        "                arr = arr[:, :, :3]\n",
        "            arr = cv2.cvtColor(arr, cv2.COLOR_BGR2RGB)\n",
        "        else:\n",
        "            arr = np.stack([arr]*3, axis=-1)\n",
        "        return Image.fromarray(arr)\n",
        "\n",
        "# -----------------------\n",
        "# Determine number of classes robustly (scan masks for unique labels)\n",
        "# -----------------------\n",
        "\n",
        "def compute_n_classes(mask_dir, sample_limit=None):\n",
        "    files = sorted([f for f in os.listdir(mask_dir) if os.path.isfile(os.path.join(mask_dir, f))])\n",
        "    if sample_limit:\n",
        "        files = files[:sample_limit]\n",
        "    uniq = set()\n",
        "    for f in files:\n",
        "        p = os.path.join(mask_dir, f)\n",
        "        try:\n",
        "            ext = os.path.splitext(p)[1].lower()\n",
        "            if ext in ('.tif', '.tiff') and _HAS_TIFFFILE:\n",
        "                arr = tifffile.imread(p)\n",
        "                if arr.ndim == 3:\n",
        "                    # if rgb-coded mask, we try first channel\n",
        "                    if arr.shape[2] in (3,4):\n",
        "                        arr = arr[..., 0]\n",
        "                    elif arr.shape[0] <= 8:\n",
        "                        arr = np.transpose(arr, (1,2,0))\n",
        "                        arr = arr[..., 0]\n",
        "                m = np.asarray(arr)\n",
        "            else:\n",
        "                m = np.array(Image.open(p).convert('L'))\n",
        "        except Exception:\n",
        "            continue\n",
        "        uniq |= set(np.unique(m).tolist())\n",
        "    uniq = sorted([int(x) for x in uniq])\n",
        "    if len(uniq) == 0:\n",
        "        raise RuntimeError(\"No labels found in masks\")\n",
        "    if uniq == list(range(0, max(uniq)+1)):\n",
        "        return max(uniq) + 1, uniq\n",
        "    return len(uniq), uniq\n",
        "\n",
        "# -----------------------\n",
        "# Dataset\n",
        "# -----------------------\n",
        "class SegmentationDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        raw_images = sorted([f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))])\n",
        "        # filter images that have masks; log missing\n",
        "        self.images = []\n",
        "        missing = []\n",
        "        for img in raw_images:\n",
        "            try:\n",
        "                _ = find_mask_for_image(img, mask_dir)\n",
        "                self.images.append(img)\n",
        "            except FileNotFoundError:\n",
        "                missing.append(img)\n",
        "        if missing:\n",
        "            print(f\"Warning: {len(missing)} images have no masks and will be skipped. Example: {missing[:5]}\")\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = find_mask_for_image(img_name, self.mask_dir)\n",
        "\n",
        "        # load image (robust)\n",
        "        try:\n",
        "            img = _load_image_with_tiff_support(img_path).convert('RGB').resize((IMG_SIZE, IMG_SIZE))\n",
        "        except Exception as e:\n",
        "            # if image can't be loaded, create placeholder and log\n",
        "            print(f\"Warning: failed to load image {img_path}: {e}. Using black placeholder.\")\n",
        "            img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0,0,0))\n",
        "\n",
        "        # load mask robustly\n",
        "        try:\n",
        "            ext = os.path.splitext(mask_path)[1].lower()\n",
        "            if ext in ('.tif', '.tiff') and _HAS_TIFFFILE:\n",
        "                m_arr = tifffile.imread(mask_path)\n",
        "                if m_arr.ndim == 3:\n",
        "                    # assume first channel encodes labels (if RGB-coded mask)\n",
        "                    if m_arr.shape[2] in (3,4):\n",
        "                        m_arr = m_arr[..., 0]\n",
        "                    else:\n",
        "                        m_arr = np.transpose(m_arr, (1,2,0))\n",
        "                        m_arr = m_arr[..., 0]\n",
        "                mask_img = Image.fromarray(m_arr)\n",
        "                mask_img = mask_img.resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
        "            else:\n",
        "                mask_img = Image.open(mask_path).convert('L').resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Failed to read mask {mask_path}: {e}\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        mask = np.array(mask_img, dtype=np.int64)\n",
        "\n",
        "        # remap raw label values -> contiguous indices\n",
        "        global label_to_index, n_classes\n",
        "        if 'label_to_index' in globals():\n",
        "            # values not found in mapping -> -1\n",
        "            vec_get = np.vectorize(lambda x: label_to_index.get(int(x), -1))\n",
        "            mask = vec_get(mask).astype(np.int64)\n",
        "            if mask.min() < 0 or (n_classes is not None and mask.max() >= n_classes):\n",
        "                raise ValueError(f\"Remapped mask index out of range for file {img_name}: min {mask.min()} max {mask.max()}\")\n",
        "        else:\n",
        "            # if mapping does not exist, assume mask already 0..n_classes-1\n",
        "            pass\n",
        "\n",
        "        return img, torch.tensor(mask, dtype=torch.long), img_name\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# build dataset (dataset will filter images without masks)\n",
        "print(\"Building dataset...\")\n",
        "dataset = SegmentationDataset(IMG_DIR, MASK_DIR, transform=transform)\n",
        "print(f\"Found {len(dataset)} usable images (images with masks).\")\n",
        "\n",
        "# compute classes (fast scan)\n",
        "try:\n",
        "    n_classes, label_values = compute_n_classes(MASK_DIR)\n",
        "    print(\"n_classes (detected):\", n_classes)\n",
        "    print(\"label values found:\", label_values)\n",
        "except Exception as e:\n",
        "    print(\"Could not auto-detect classes:\", e)\n",
        "    if CLASS_NAMES is not None:\n",
        "        n_classes = len(CLASS_NAMES)\n",
        "        label_values = list(range(n_classes))\n",
        "        print(f\"Falling back to n_classes={n_classes} from CLASS_NAMES\")\n",
        "    else:\n",
        "        n_classes = 2\n",
        "        label_values = [0,1]\n",
        "        print(\"Falling back to n_classes=2\")\n",
        "\n",
        "# build mapping raw->index\n",
        "label_to_index = {raw: idx for idx, raw in enumerate(label_values)}\n",
        "index_to_label = {idx: raw for raw, idx in label_to_index.items()}\n",
        "print(\"label_to_index mapping:\", label_to_index)\n",
        "\n",
        "# reconcile CLASS_NAMES ordering with detected labels\n",
        "if CLASS_NAMES is not None and len(CLASS_NAMES) >= n_classes:\n",
        "    CLASS_NAMES = CLASS_NAMES[:n_classes]\n",
        "else:\n",
        "    CLASS_NAMES = [str(v) for v in label_values]\n",
        "print(\"Using class names:\", CLASS_NAMES)\n",
        "\n",
        "# split\n",
        "if len(dataset) <= 1:\n",
        "    print(\"Single-image dataset detected, skipping train/val split.\")\n",
        "    train_dataset, val_dataset = [], dataset\n",
        "else:\n",
        "    val_size = max(1, int(len(dataset) * VAL_SPLIT))\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "if train_dataset:\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              pin_memory=(device.type==\"cuda\"), num_workers=NUM_WORKERS)\n",
        "else:\n",
        "    train_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                              pin_memory=(device.type==\"cuda\"), num_workers=NUM_WORKERS)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                        pin_memory=(device.type==\"cuda\"), num_workers=NUM_WORKERS)\n",
        "\n",
        "# -----------------------\n",
        "# Model: U-Net (same architecture but smaller init if memory tight)\n",
        "# -----------------------\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_c, out_c):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(in_c, out_c, 3, padding=1), nn.BatchNorm2d(out_c), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_c, out_c, 3, padding=1), nn.BatchNorm2d(out_c), nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_classes, base_filters=64):\n",
        "        super().__init__()\n",
        "        f = base_filters\n",
        "        self.down1 = DoubleConv(3, f)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.down2 = DoubleConv(f, f*2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.down3 = DoubleConv(f*2, f*4)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.down4 = DoubleConv(f*4, f*8)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.bottleneck = DoubleConv(f*8, f*16)\n",
        "\n",
        "        self.up4 = nn.ConvTranspose2d(f*16, f*8, 2, stride=2)\n",
        "        self.conv4 = DoubleConv(f*16, f*8)\n",
        "        self.up3 = nn.ConvTranspose2d(f*8, f*4, 2, stride=2)\n",
        "        self.conv3 = DoubleConv(f*8, f*4)\n",
        "        self.up2 = nn.ConvTranspose2d(f*4, f*2, 2, stride=2)\n",
        "        self.conv2 = DoubleConv(f*4, f*2)\n",
        "        self.up1 = nn.ConvTranspose2d(f*2, f, 2, stride=2)\n",
        "        self.conv1 = DoubleConv(f*2, f)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(f, n_classes, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.down1(x); p1 = self.pool1(d1)\n",
        "        d2 = self.down2(p1); p2 = self.pool2(d2)\n",
        "        d3 = self.down3(p2); p3 = self.pool3(d3)\n",
        "        d4 = self.down4(p3); p4 = self.pool4(d4)\n",
        "        bn = self.bottleneck(p4)\n",
        "\n",
        "        up4 = self.up4(bn); c4 = self.conv4(torch.cat([up4, d4], 1))\n",
        "        up3 = self.up3(c4); c3 = self.conv3(torch.cat([up3, d3], 1))\n",
        "        up2 = self.up2(c3); c2 = self.conv2(torch.cat([up2, d2], 1))\n",
        "        up1 = self.up1(c2); c1 = self.conv1(torch.cat([up1, d1], 1))\n",
        "\n",
        "        return self.out_conv(c1)\n",
        "\n",
        "# auto-scale base filters based on available GPU memory to avoid OOM\n",
        "try:\n",
        "    if device.type == 'cuda':\n",
        "        props = torch.cuda.get_device_properties(0)\n",
        "        vram_gb = props.total_memory / 1e9\n",
        "    else:\n",
        "        vram_gb = 4\n",
        "except Exception:\n",
        "    vram_gb = 4\n",
        "\n",
        "base_filters = 64 if vram_gb > 6 else 32 if vram_gb > 3 else 16\n",
        "model = UNet(n_classes, base_filters=base_filters).to(device)\n",
        "print(f\"UNet initialized with base_filters={base_filters}\")\n",
        "\n",
        "# -----------------------\n",
        "# Loss + Optimizer\n",
        "# -----------------------\n",
        "if n_classes == 1:\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=3)\n",
        "scaler = torch.cuda.amp.GradScaler() if device.type==\"cuda\" else None\n",
        "\n",
        "# checkpoint path\n",
        "ckpt_path = os.path.join(OUT_DIR, \"checkpoint.pth\")\n",
        "best_path = os.path.join(OUT_DIR, \"best_unet.pth\")\n",
        "start_epoch = 0\n",
        "best_val = float(\"inf\")\n",
        "\n",
        "# try resume\n",
        "if os.path.exists(ckpt_path):\n",
        "    try:\n",
        "        chk = torch.load(ckpt_path, map_location=device)\n",
        "        model.load_state_dict(chk['model'])\n",
        "        optimizer.load_state_dict(chk['optim'])\n",
        "        start_epoch = chk.get('epoch', 0) + 1\n",
        "        best_val = chk.get('best_val', best_val)\n",
        "        print(f\"Resumed from checkpoint epoch {start_epoch}\")\n",
        "    except Exception as e:\n",
        "        print(\"Warning: failed to load checkpoint:\", e)\n",
        "\n",
        "# -----------------------\n",
        "# Small utilities for metrics\n",
        "# -----------------------\n",
        "\n",
        "def fast_confusion_matrix(gt, pred, n_classes):\n",
        "    \"\"\"Memory-efficient confusion matrix using bincount. gt and pred are 1D arrays of same length.\"\"\"\n",
        "    assert gt.shape == pred.shape\n",
        "    mask = (gt >= 0) & (gt < n_classes)\n",
        "    if mask.sum() == 0:\n",
        "        return np.zeros((n_classes, n_classes), dtype=np.int64)\n",
        "    combined = n_classes * gt[mask].astype(int) + pred[mask].astype(int)\n",
        "    cm = np.bincount(combined, minlength=n_classes**2).reshape(n_classes, n_classes)\n",
        "    return cm\n",
        "\n",
        "\n",
        "def compute_metrics_per_class(gt, pred, n_classes):\n",
        "    eps = 1e-8\n",
        "    ious, dices, precisions, recalls, f1s = [], [], [], [], []\n",
        "    for c in range(n_classes):\n",
        "        gt_c = (gt == c)\n",
        "        pred_c = (pred == c)\n",
        "        intersection = (gt_c & pred_c).sum()\n",
        "        union = (gt_c | pred_c).sum()\n",
        "        tp = intersection\n",
        "        fp = (pred_c & ~gt_c).sum()\n",
        "        fn = (gt_c & ~pred_c).sum()\n",
        "\n",
        "        iou = (intersection + eps) / (union + eps)\n",
        "        dice = (2 * intersection + eps) / (gt_c.sum() + pred_c.sum() + eps)\n",
        "        precision = (tp + eps) / (tp + fp + eps)\n",
        "        recall = (tp + eps) / (tp + fn + eps)\n",
        "        f1 = (2 * precision * recall + eps) / (precision + recall + eps)\n",
        "\n",
        "        ious.append(iou)\n",
        "        dices.append(dice)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1s.append(f1)\n",
        "    return {\n",
        "        'iou': np.array(ious),\n",
        "        'dice': np.array(dices),\n",
        "        'precision': np.array(precisions),\n",
        "        'recall': np.array(recalls),\n",
        "        'f1': np.array(f1s)\n",
        "    }\n",
        "\n",
        "# -----------------------\n",
        "# Training\n",
        "# -----------------------\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(start_epoch, N_EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    optimizer.zero_grad()\n",
        "    nb_batches = max(1, len(train_loader))\n",
        "\n",
        "    pbar = tqdm(enumerate(train_loader), total=nb_batches, desc=f\"Epoch [{epoch+1}/{N_EPOCHS}]\")\n",
        "    for step, batch in pbar:\n",
        "        if len(batch) == 3:\n",
        "            imgs, masks, _ = batch\n",
        "        else:\n",
        "            imgs, masks = batch\n",
        "        imgs, masks = imgs.to(device), masks.to(device)\n",
        "\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                outputs = model(imgs)\n",
        "                # BCE expects shape [B,1,H,W] / float masks; CrossEntropy expects [B,C,H,W] logits and Long masks\n",
        "                if n_classes == 1:\n",
        "                    outputs = outputs.squeeze(1)\n",
        "                    loss = criterion(outputs, masks.float())\n",
        "                else:\n",
        "                    loss = criterion(outputs, masks)\n",
        "                loss = loss / ACCUM_STEPS\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            outputs = model(imgs)\n",
        "            if n_classes == 1:\n",
        "                outputs = outputs.squeeze(1)\n",
        "                loss = criterion(outputs, masks.float())\n",
        "            else:\n",
        "                loss = criterion(outputs, masks)\n",
        "            loss = loss / ACCUM_STEPS\n",
        "            loss.backward()\n",
        "\n",
        "        if (step + 1) % ACCUM_STEPS == 0:\n",
        "            if scaler is not None:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        running_loss += (loss.item() * ACCUM_STEPS)\n",
        "        pbar.set_postfix(loss=f\"{loss.item()*ACCUM_STEPS:.4f}\")\n",
        "\n",
        "    # step remaining grads\n",
        "    if (nb_batches % ACCUM_STEPS) != 0:\n",
        "        if scaler is not None:\n",
        "            scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n",
        "        else:\n",
        "            optimizer.step(); optimizer.zero_grad()\n",
        "\n",
        "    avg_train = running_loss / max(1, nb_batches)\n",
        "    train_losses.append(avg_train)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    all_preds, all_gts, all_names = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            if len(batch) == 3:\n",
        "                imgs, masks, names = batch\n",
        "            else:\n",
        "                imgs, masks = batch\n",
        "                names = [\"unknown\"] * imgs.shape[0]\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "\n",
        "            if scaler is not None:\n",
        "                with torch.cuda.amp.autocast(enabled=True):\n",
        "                    outputs = model(imgs)\n",
        "                    if n_classes == 1:\n",
        "                        outputs_proc = outputs.squeeze(1)\n",
        "                        loss = criterion(outputs_proc, masks.float())\n",
        "                        preds = (torch.sigmoid(outputs_proc) > 0.5).long().cpu().numpy()\n",
        "                    else:\n",
        "                        loss = criterion(outputs, masks)\n",
        "                        preds = outputs.argmax(1).cpu().numpy()\n",
        "            else:\n",
        "                outputs = model(imgs)\n",
        "                if n_classes == 1:\n",
        "                    outputs_proc = outputs.squeeze(1)\n",
        "                    loss = criterion(outputs_proc, masks.float())\n",
        "                    preds = (torch.sigmoid(outputs_proc) > 0.5).long().cpu().numpy()\n",
        "                else:\n",
        "                    loss = criterion(outputs, masks)\n",
        "                    preds = outputs.argmax(1).cpu().numpy()\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            all_preds.append(preds)\n",
        "            all_gts.append(masks.cpu().numpy())\n",
        "            all_names.extend(names)\n",
        "\n",
        "    avg_val = val_loss / max(1, len(val_loader))\n",
        "    val_losses.append(avg_val)\n",
        "    scheduler.step(avg_val)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} — Train: {avg_train:.4f}  Val: {avg_val:.4f}\")\n",
        "\n",
        "    # checkpoint\n",
        "    try:\n",
        "        torch.save({'model': model.state_dict(), 'optim': optimizer.state_dict(), 'epoch': epoch, 'best_val': best_val}, ckpt_path)\n",
        "    except Exception as e:\n",
        "        print(\"Could not write checkpoint:\", e)\n",
        "\n",
        "    if avg_val < best_val:\n",
        "        best_val = avg_val\n",
        "        try:\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print(\"✅ Saved best model.\")\n",
        "        except Exception as e:\n",
        "            print(\"Could not save best model:\", e)\n",
        "\n",
        "    torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "# -----------------------\n",
        "# Loss curve (log-scale) - safe plotting\n",
        "# -----------------------\n",
        "plt.figure()\n",
        "if train_losses:\n",
        "    plt.semilogy(train_losses, label=\"Train Loss\")\n",
        "if val_losses:\n",
        "    plt.semilogy(val_losses, label=\"Val Loss\")\n",
        "plt.legend(); plt.xlabel(\"Epoch\"); plt.ylabel(\"Log Loss\")\n",
        "plt.title(\"Log-scaled Loss Curve\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"log_loss_curve.png\"))\n",
        "plt.close()\n",
        "\n",
        "# -----------------------\n",
        "# Evaluation (collect preds with filenames)\n",
        "# -----------------------\n",
        "state_path = best_path if os.path.exists(best_path) else ckpt_path if os.path.exists(ckpt_path) else None\n",
        "if state_path is not None:\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(state_path, map_location=device))\n",
        "        print(f\"Loaded model weights from {state_path}\")\n",
        "    except Exception as e:\n",
        "        print(\"Warning: could not load saved weights:\", e)\n",
        "else:\n",
        "    print(\"Warning: no saved model file found; using current weights\")\n",
        "\n",
        "model.eval()\n",
        "all_preds, all_gts = [], []\n",
        "all_names = []\n",
        "\n",
        "if len(val_loader) > 0:\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            if len(batch) == 3:\n",
        "                imgs, masks, names = batch\n",
        "            else:\n",
        "                imgs, masks = batch\n",
        "                names = [\"unknown\"] * imgs.shape[0]\n",
        "            imgs, masks = imgs.to(device), masks.to(device)\n",
        "            if scaler is not None:\n",
        "                with torch.cuda.amp.autocast(enabled=True):\n",
        "                    outputs = model(imgs)\n",
        "            else:\n",
        "                outputs = model(imgs)\n",
        "\n",
        "            if n_classes == 1:\n",
        "                preds = (torch.sigmoid(outputs.squeeze(1)) > 0.5).long().cpu().numpy()\n",
        "            else:\n",
        "                preds = outputs.argmax(1).cpu().numpy()\n",
        "\n",
        "            all_preds.append(preds)\n",
        "            all_gts.append(masks.cpu().numpy())\n",
        "            all_names.extend(names)\n",
        "\n",
        "if all_preds:\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "    all_gts   = np.concatenate(all_gts, axis=0)\n",
        "else:\n",
        "    print(\"⚠️ No predictions collected. Skipping metrics/overlays.\")\n",
        "    all_preds = np.array([])\n",
        "    all_gts = np.array([])\n",
        "\n",
        "# -----------------------\n",
        "# Confusion matrix (flattened) and normalized\n",
        "# -----------------------\n",
        "if all_preds.size and all_gts.size:\n",
        "    cm = fast_confusion_matrix(all_gts.flatten(), all_preds.flatten(), n_classes)\n",
        "    cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "    plt.title('Confusion Matrix (counts)')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(range(n_classes), CLASS_NAMES, rotation=45, ha='right')\n",
        "    plt.yticks(range(n_classes), CLASS_NAMES)\n",
        "    for i in range(n_classes):\n",
        "        for j in range(n_classes):\n",
        "            plt.text(j, i, f\"{cm[i,j]}\", ha='center', va='center', color='black')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, 'confusion_matrix_counts.png'))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.imshow(cm_norm, interpolation='nearest', cmap='coolwarm', vmin=0, vmax=1)\n",
        "    plt.title('Confusion Matrix (normalized by GT)')\n",
        "    plt.colorbar()\n",
        "    plt.xticks(range(n_classes), CLASS_NAMES, rotation=45, ha='right')\n",
        "    plt.yticks(range(n_classes), CLASS_NAMES)\n",
        "    for i in range(n_classes):\n",
        "        for j in range(n_classes):\n",
        "            plt.text(j, i, f\"{cm_norm[i,j]:.2f}\", ha='center', va='center', color='black')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, 'confusion_matrix_normalized.png'))\n",
        "    plt.close()\n",
        "else:\n",
        "    print(\"Skipping confusion matrix: no valid predictions or ground-truths.\")\n",
        "\n",
        "# -----------------------\n",
        "# Per-class metrics (IoU, Dice, Precision, Recall, F1)\n",
        "# -----------------------\n",
        "if all_preds.size and all_gts.size:\n",
        "    metrics = compute_metrics_per_class(all_gts.flatten(), all_preds.flatten(), n_classes)\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'class': CLASS_NAMES,\n",
        "        'iou': metrics['iou'],\n",
        "        'dice': metrics['dice'],\n",
        "        'precision': metrics['precision'],\n",
        "        'recall': metrics['recall'],\n",
        "        'f1': metrics['f1']\n",
        "    })\n",
        "    metrics_df.to_csv(os.path.join(OUT_DIR, 'per_class_metrics.csv'), index=False)\n",
        "    print('Saved per-class metrics to', os.path.join(OUT_DIR, 'per_class_metrics.csv'))\n",
        "\n",
        "    # Plot per-class IoU + Dice\n",
        "    x = np.arange(len(CLASS_NAMES))\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(x, metrics_df['iou'], marker='o', label='IoU')\n",
        "    plt.plot(x, metrics_df['dice'], marker='o', label='Dice')\n",
        "    plt.xticks(x, CLASS_NAMES, rotation=45, ha='right')\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('Per-class IoU and Dice')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, 'per_class_iou_dice.png'))\n",
        "    plt.close()\n",
        "else:\n",
        "    print('Skipping per-class metrics: no predictions.')\n",
        "\n",
        "# meters per pixel (adjust based on dataset metadata, e.g. 10 for Sentinel-2)\n",
        "PIXEL_SIZE_M = 10\n",
        "\n",
        "# -----------------------\n",
        "# Per-image area area in meter square (gis data stanadpoint) and correlation heatmap\n",
        "# ----------------------\n",
        "if all_preds.size and all_gts.size and len(all_names) == all_gts.shape[0]:\n",
        "    area_rows = []\n",
        "    for i, name in enumerate(all_names):\n",
        "        gt = all_gts[i]\n",
        "        total_pixels = gt.size\n",
        "        area_row = {'name': name}\n",
        "        for c in range(n_classes):\n",
        "            pixel_count = (gt == c).sum()\n",
        "            area_m2 = int(pixel_count) * (PIXEL_SIZE_M ** 2)\n",
        "            area_row[f'gt_area_m2_{CLASS_NAMES[c]}'] = area_m2\n",
        "        area_rows.append(area_row)\n",
        "\n",
        "    area_df = pd.DataFrame(area_rows)\n",
        "    area_df.to_csv(os.path.join(OUT_DIR, 'per_image_area_m2.csv'), index=False)\n",
        "    print('Saved per-image area (m²) to', os.path.join(OUT_DIR, 'per_image_area_m2.csv'))\n",
        "\n",
        "    # Compute per-image class fractions\n",
        "    per_image_df = area_df.copy()\n",
        "    total_cols = [f'gt_area_m2_{cn}' for cn in CLASS_NAMES]\n",
        "    per_image_df['total_area'] = per_image_df[total_cols].sum(axis=1)\n",
        "    for c in range(n_classes):\n",
        "        col = f'gt_area_m2_{CLASS_NAMES[c]}'\n",
        "        per_image_df[f'gt_area_frac_{CLASS_NAMES[c]}'] = per_image_df[col] / (per_image_df['total_area'] + 1e-12)\n",
        "\n",
        "    gt_frac_cols = [c for c in per_image_df.columns if c.startswith('gt_area_frac_')]\n",
        "    gt_frac_matrix = per_image_df[gt_frac_cols].corr()\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.imshow(gt_frac_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.colorbar()\n",
        "    plt.xticks(range(len(gt_frac_cols)), [c.replace('gt_area_frac_','') for c in gt_frac_cols], rotation=45, ha='right')\n",
        "    plt.yticks(range(len(gt_frac_cols)), [c.replace('gt_area_frac_','') for c in gt_frac_cols])\n",
        "    plt.title('Correlation (GT area fractions across images)')\n",
        "    for i in range(len(gt_frac_cols)):\n",
        "        for j in range(len(gt_frac_cols)):\n",
        "            plt.text(j, i, f\"{gt_frac_matrix.values[i,j]:.2f}\", ha='center', va='center', color='black')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT_DIR, 'gt_area_fraction_correlation.png'))\n",
        "    plt.close()\n",
        "else:\n",
        "    print('Skipping per-image area/correlation: insufficient data.')\n",
        "\n",
        "# -----------------------\n",
        "# Palette and visualization helpers\n",
        "# -----------------------\n",
        "\n",
        "def make_palette():\n",
        "    fixed_palette = {\n",
        "        \"background\": (0, 0, 0),         # black\n",
        "        \"water\": (114, 151, 158),        # teal\n",
        "        \"land\": (240, 123, 107),         # reddish\n",
        "        \"vegetation\": (58, 59, 23),      # dark green\n",
        "        \"barren\": (120, 113, 63),        # olive brown\n",
        "        \"built_up\": (200, 200, 200)      # light gray (fallback if needed)\n",
        "    }\n",
        "    pal = np.zeros((n_classes, 3), dtype=np.uint8)\n",
        "    for i, cname in enumerate(CLASS_NAMES):\n",
        "        pal[i] = fixed_palette.get(cname, (np.random.randint(0,255),\n",
        "                                           np.random.randint(0,255),\n",
        "                                           np.random.randint(0,255)))\n",
        "    return pal\n",
        "\n",
        "\n",
        "PALETTE = make_palette()\n",
        "\n",
        "\n",
        "def colorize_mask(mask, palette=PALETTE):\n",
        "    mask = np.asarray(mask, dtype=np.int64)\n",
        "    h,w = mask.shape\n",
        "    colored = np.zeros((h,w,3), dtype=np.uint8)\n",
        "    for c in range(len(palette)):\n",
        "        colored[mask==c] = palette[c]\n",
        "    return colored\n",
        "\n",
        "def overlay_on_image(img, mask_colored, alpha=0.5):\n",
        "    base = np.array(img).astype(np.float32)/255.0\n",
        "    mask_rgb = mask_colored.astype(np.float32)/255.0\n",
        "    overlay = (1-alpha)*base + alpha*mask_rgb\n",
        "    overlay = np.clip(overlay*255,0,255).astype(np.uint8)\n",
        "    return overlay\n",
        "\n",
        "# -----------------------\n",
        "# Side-by-side visualization with overlay and legend\n",
        "# -----------------------\n",
        "\n",
        "def show_examples_with_gt_overlay(img_dir, names, preds, gts, n=5):\n",
        "    n = min(n, len(names))\n",
        "    for i in range(n):\n",
        "        img_name = names[i]\n",
        "        img_path = os.path.join(img_dir, img_name)\n",
        "        try:\n",
        "            img = _load_image_with_tiff_support(img_path).convert(\"RGB\").resize((IMG_SIZE, IMG_SIZE))\n",
        "        except Exception:\n",
        "            img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0,0,0))\n",
        "        img_np = np.array(img)\n",
        "\n",
        "        pred = preds[i]\n",
        "        gt = gts[i]\n",
        "\n",
        "        # color masks\n",
        "        colored_gt = colorize_mask(gt)\n",
        "        colored_pred = colorize_mask(pred)\n",
        "        overlay_pred = overlay_on_image(img_np, colored_pred, alpha=0.5)\n",
        "\n",
        "        # figure with 4 subplots on top row\n",
        "        fig = plt.figure(figsize=(16,6))\n",
        "        gs = fig.add_gridspec(2, 4, height_ratios=[4, 1])\n",
        "\n",
        "        ax0 = fig.add_subplot(gs[0,0]); ax0.imshow(img_np); ax0.set_title(\"Input\"); ax0.axis('off')\n",
        "        ax1 = fig.add_subplot(gs[0,1]); ax1.imshow(colored_gt); ax1.set_title(\"Ground Truth\"); ax1.axis('off')\n",
        "        ax2 = fig.add_subplot(gs[0,2]); ax2.imshow(colored_pred); ax2.set_title(\"Predicted\"); ax2.axis('off')\n",
        "        ax3 = fig.add_subplot(gs[0,3]); ax3.imshow(overlay_pred); ax3.set_title(\"Overlay\"); ax3.axis('off')\n",
        "\n",
        "        # bottom row: legend\n",
        "        legend_ax = fig.add_subplot(gs[1,:])\n",
        "        legend_ax.axis('off')\n",
        "        n_classes_local = len(CLASS_NAMES)\n",
        "        for k, cname in enumerate(CLASS_NAMES):\n",
        "            x0 = k * (1.0 / n_classes_local)\n",
        "            rect = plt.Rectangle((x0, 0.2), 1.0 / n_classes_local * 0.8, 0.6, facecolor=np.array(PALETTE[k])/255.0)\n",
        "            legend_ax.add_patch(rect)\n",
        "            legend_ax.text(x0 + 0.01, 0.85, cname, transform=legend_ax.transAxes, fontsize=10)\n",
        "\n",
        "        plt.suptitle(f\"Example {i+1}: {img_name}\", fontsize=14)\n",
        "        plt.tight_layout(rect=[0,0.05,1,0.95])\n",
        "        fname = os.path.join(OUT_DIR, f\"example_overlay_{i+1}_{img_name}.png\")\n",
        "        try:\n",
        "            plt.savefig(fname)\n",
        "        except Exception:\n",
        "            # Try safer filename\n",
        "            safe_name = f\"example_overlay_{i+1}.png\"\n",
        "            plt.savefig(os.path.join(OUT_DIR, safe_name))\n",
        "        plt.close(fig)\n",
        "\n",
        "print(\"Generating side-by-side overlays with GT, Predicted, and Overlay...\")\n",
        "try:\n",
        "    show_examples_with_gt_overlay(IMG_DIR, all_names, all_preds, all_gts, n=5)\n",
        "    print(\"✅ Saved example overlays to\", OUT_DIR)\n",
        "except Exception as e:\n",
        "    print(\"Could not generate overlays:\", e)\n",
        "\n",
        "# -----------------------\n",
        "# Save training log CSV\n",
        "# -----------------------\n",
        "try:\n",
        "    log_df = pd.DataFrame({'epoch': list(range(1, len(train_losses)+1)), 'train_loss': train_losses, 'val_loss': val_losses})\n",
        "    log_df.to_csv(os.path.join(OUT_DIR, 'training_log.csv'), index=False)\n",
        "    print('Saved training log to', os.path.join(OUT_DIR, 'training_log.csv'))\n",
        "except Exception as e:\n",
        "    print('Could not save training log:', e)\n",
        "\n",
        "print('Pipeline finished.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** UTILS **  "
      ],
      "metadata": {
        "id": "a2AiSZ6PWlpN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "zfh17zNqp1Gx",
        "outputId": "5daa8f2c-a287-459a-e027-d132c2917f43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.35.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.6.15 in /usr/local/lib/python3.12/dist-packages (from selenium) (2025.8.3)\n",
            "Collecting typing_extensions~=4.14.0 (from selenium)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.35.0-py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, typing_extensions, outcome, trio, trio-websocket, selenium\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.15.0\n",
            "    Uninstalling typing_extensions-4.15.0:\n",
            "      Successfully uninstalled typing_extensions-4.15.0\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.35.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.14.1 wsproto-1.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "ae0dd98128804e86926b17b9d76436eb",
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0i2Gv7eD1T1"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "import time\n",
        "\n",
        "# Start Chrome\n",
        "driver = webdriver.Chrome()\n",
        "driver.get(\"https://example.com\")  # replace with the site you want\n",
        "\n",
        "while True:\n",
        "    driver.execute_script(\"window.scrollBy(0, window.innerHeight/3);\")\n",
        "    time.sleep(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvZ_U-MrD1Ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76c53e58-cd94-41f1-e132-045f1c51a3f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "end of segemnetaion\n"
          ]
        }
      ],
      "source": [
        "print(\"end of segemnetaion\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNSTxQ5GD1O2"
      },
      "outputs": [],
      "source": [
        "# optional fast api backend"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLE1XK7bD1I4"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import os, shutil\n",
        "\n",
        "# Create folders for uploads\n",
        "os.makedirs(\"dataset/images\", exist_ok=True)\n",
        "os.makedirs(\"dataset/masks\", exist_ok=True)\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI(title=\"Segmentation Backend\")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"message\": \"Segmentation API is running!\"}\n",
        "\n",
        "@app.post(\"/upload/image/\")\n",
        "async def upload_image(file: UploadFile = File(...)):\n",
        "    save_path = os.path.join(\"dataset/images\", file.filename)\n",
        "    with open(save_path, \"wb\") as buffer:\n",
        "        shutil.copyfileobj(file.file, buffer)\n",
        "    return {\"status\": \"image uploaded\", \"path\": save_path}\n",
        "\n",
        "@app.post(\"/upload/mask/\")\n",
        "async def upload_mask(file: UploadFile = File(...)):\n",
        "    save_path = os.path.join(\"dataset/masks\", file.filename)\n",
        "    with open(save_path, \"wb\") as buffer:\n",
        "        shutil.copyfileobj(file.file, buffer)\n",
        "    return {\"status\": \"mask uploaded\", \"path\": save_path}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPqADRQPD1L9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1dc659e-07ad-4bf8-aeec-3ee80bf2a47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.116.2)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.35.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.48.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.9)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.4.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pyngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nM3kZRp58E1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hWnxz2mf8EyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T6OGlLqv8Eu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agRNPlclD1Fi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpVQvOBwD1CO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph6oXwQ9D0-r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLVyWbt6D07H"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1tihO-VD030"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cyNZ4ZJD00f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBiNXgp4D0xb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W246Xq1qD0uN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiw7lZTgD0rK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsBYPFz7D0oV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv9g-KjvD0lE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY6eGnv9D0iB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpbV5mJ1D0fU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TojeDKsmD0cg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6GqlMtGD0Zd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVPbA9QCD0Wh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLwbsCXTD0Tb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7e8d9jdD0Qj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyv9_mrpD0Nw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tqr9MPaKD0K6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MHF-3mED0H_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD6JF-J-D0FD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GTW8HLbD0B2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wjYR-J4Du9k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQkmgx3NGE8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g88nEV_SGE44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ElWGgSxnGE1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2n_QdJTGGEzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aOXI9e5iGEvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F9-VqS2JGEs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OSHDkG4xGEqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R5OeLQ1mGEmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LPo6tXMxfSn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NCM5Fmz_fSki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zm0SouTufSiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AtAtMNk-fSfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hI2hiNWofSdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSEZdWt4fSaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e71F4l8ZfSXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sauzy9kIfSUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWrakH-JfSRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9XmBbrMtfSOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H1sF2MS2fSLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rbLFe3eKfSIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RA15kicfSE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4cqvLlUmfSBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qweslg3efR-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gH8nxkMBfR7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2fKf9YeLfR4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "16vhCMb1fR1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RCm278h4fRyB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO2LkWR1UL7iScWDcNMOVqr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}